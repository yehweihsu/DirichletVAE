# -*- coding: utf-8 -*-
"""sepsis_vae.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G9uOfp3pgjX_pop3B3bc_ykqtRZHpTb2
"""

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
os.environ['TORCH_USE_CUDA_DSA'] = '1'

from __future__ import print_function
from multiprocessing.dummy import Pool as ThreadPool
import torch
import torch.utils.data
from torch import nn, optim
from torch.nn import functional as F
from torchvision import datasets, transforms
from torchvision.utils import save_image
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

ngf = 64
ndf = 64
nc = 1

TRAIN_RATIO = 0.8
BATCH_SIZE = 256 # You can adjust this batch size
RND_SEED = 1234
NUM_EPOCHES = 20

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

np.random.seed(RND_SEED)
torch.manual_seed(RND_SEED)
if torch.cuda.is_available():
  torch.cuda.manual_seed(RND_SEED)



# Assuming 'tensor' is your data tensor
# Define the split ratio (e.g., 80% train, 20% test)

dataset_size = len(features)
train_size = int(TRAIN_RATIO * dataset_size)
test_size = dataset_size - train_size

# Split the dataset
print(features.dtype)
dataset = TensorDataset(features)
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# Create data loaders

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

def prior(K, alpha):
    """
    Prior for the model.
    :K: number of categories
    :alpha: Hyper param of Dir
    :return: mean and variance tensors
    """
    # ラプラス近似で正規分布に近似
    # Approximate to normal distribution using Laplace approximation
    a = torch.Tensor(1, K).float().fill_(alpha)
    mean = a.log().t() - a.log().mean(1)
    var = ((1 - 2.0 / K) * a.reciprocal()).t() + (1.0 / K ** 2) * a.reciprocal().sum(1)
    return mean.t(), var.t() # Parameters of prior distribution after approximation

class Dir_VAE(nn.Module):
    def __init__(self, input_size,latent_size=10, hidden_dim = 200):
        self.latent_size = latent_size
        self.hidden_dim = hidden_dim
        self.input_size = input_size
        super(Dir_VAE, self).__init__()

        self.encoder = nn.Sequential(
          # nn.Flatten(), # Removed Flatten as input is already flattened
          nn.Linear(self.input_size, self.hidden_dim),
          nn.ReLU(),
          nn.Linear(self.hidden_dim, self.hidden_dim),
          nn.ReLU(),
          nn.Linear(self.hidden_dim, self.hidden_dim),
          nn.ReLU()
        )
        self.decoder = nn.Sequential(
          nn.Linear(self.latent_size, self.hidden_dim),
          nn.ReLU(),
          nn.Linear(self.hidden_dim, self.hidden_dim),
          nn.ReLU(),
          nn.Linear(self.hidden_dim, self.hidden_dim),
          nn.ReLU(),
          nn.Linear(self.hidden_dim, self.input_size),
          nn.Sigmoid(),
          # nn.Unflatten(dim=1, unflattened_size=(1, 28, 28)) # Removed Unflatten
        )

        self.fc21 = nn.Linear(self.hidden_dim, self.latent_size)
        self.fc22 = nn.Linear(self.hidden_dim, self.latent_size)

        self.lrelu = nn.LeakyReLU()
        self.relu = nn.ReLU()

        # Dir prior
        self.prior_mean, self.prior_var = map(nn.Parameter, prior(10, 0.3)) # 0.3 is a hyper param of Dirichlet distribution
        self.prior_logvar = nn.Parameter(self.prior_var.log())
        self.prior_mean.requires_grad = False
        self.prior_var.requires_grad = False
        self.prior_logvar.requires_grad = False


    def encode(self, x):
        encoding = self.encoder(x);
        return self.fc21(encoding), self.fc22(encoding)

    def decode(self, gauss_z):
        dir_z = F.softmax(gauss_z,dim=1)
        return self.decoder(dir_z)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5*logvar)
        eps = torch.randn_like(std)
        return mu + eps*std


    def forward(self, x):
        mu, logvar = self.encode(x)
        gauss_z = self.reparameterize(mu, logvar)
        dir_z = F.softmax(gauss_z,dim=1)
        return self.decode(gauss_z), mu, logvar, gauss_z, dir_z

    # Reconstruction + KL divergence losses summed over all elements and batch
    def loss_function(self, recon_x, x, mu, logvar, K):
        beta = 1.0
        # Adjusted BCE to use self.input_size instead of 784
        BCE = F.binary_cross_entropy(recon_x.view(-1, self.input_size), x.view(-1, self.input_size), reduction='sum')
        # ディリクレ事前分布と変分事後分布とのKLを計算
        # Calculating KL with Dirichlet prior and variational posterior distributions
        # Original paper:"Autoencodeing variational inference for topic model"-https://arxiv.org/pdf/1703.01488
        prior_mean = self.prior_mean.expand_as(mu)
        prior_var = self.prior_var.expand_as(logvar)
        prior_logvar = self.prior_logvar.expand_as(logvar)
        var_division = logvar.exp() / prior_var # Σ_0 / Σ_1
        diff = mu - prior_mean # μ_１ - μ_0
        diff_term = diff *diff / prior_var # (μ_1 - μ_0)(μ_1 - μ_0)/Σ_1
        logvar_division = prior_logvar - logvar # log|Σ_1| - log|Σ_0| = log(|Σ_1|/|Σ_2|)
        # KL
        KLD = 0.5 * ((var_division + diff_term + logvar_division).sum(1) - K)
        return BCE + KLD

input_size = INPUT_SIZE #mnist dimensions, to change
model = Dir_VAE(input_size).to(device)
optimizer = optim.AdamW(model.parameters(), lr=1e-4)

def train(epoch):
    model.train()
    train_loss = 0
    for batch_idx, (data,) in enumerate(train_loader): # Modified unpacking
        data = data.to(device)
        optimizer.zero_grad()
        recon_batch, mu, logvar, gauss_z, dir_z = model(data)

        loss = model.loss_function(recon_batch, data, mu, logvar, 10)
        loss = loss.mean()
        loss.backward()
        train_loss += loss.item()
        optimizer.step()
        if batch_idx % 10 == 0:
            #print(f"gause_z:{gauss_z[0]}")
            #print(f"dir_z:{dir_z[0]},SUM:{torch.sum(dir_z[0])}")
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader),
                loss.item() / len(data)))

    print('====> Epoch: {} Average loss: {:.4f}'.format(
          epoch, train_loss / len(train_loader.dataset)))

def test(epoch):
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for i, (data,) in enumerate(test_loader): # Modified unpacking
            data = data.to(device)
            recon_batch, mu, logvar, gauss_z, dir_z = model(data)
            loss = model.loss_function(recon_batch, data, mu, logvar, 10)
            test_loss += loss.mean()
            test_loss.item()
            if i == 0:
                n = min(data.size(0), 18)
                #comparison = torch.cat([data[:n],
                #                      recon_batch.view(args.batch_size, 1, 28, 28)[:n]])
                #save_image(comparison.cpu(),
                #         'image/recon_' + str(epoch) + '.png', nrow=n)

    test_loss /= len(test_loader.dataset)
    print('====> Test set loss: {:.4f}'.format(test_loss))